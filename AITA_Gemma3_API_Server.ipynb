{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "654232cd",
   "metadata": {},
   "source": [
    "# AITA Travel AI Chat - Gemma 3 API Server\n",
    "\n",
    "This notebook sets up a FastAPI server with Gemma 3 model to power the AITA travel chat assistant.\n",
    "\n",
    "## How to Use This Notebook in Google Colab\n",
    "\n",
    "### Step 1: Opening Google Colab\n",
    "1. Go to [colab.research.google.com](https://colab.research.google.com)\n",
    "2. Sign in with your Google account\n",
    "3. Click \"New notebook\" or upload this existing notebook\n",
    "\n",
    "### Step 2: Naming Your Notebook\n",
    "- Click on the notebook name at the top (usually \"Untitled\")\n",
    "- Rename it to something descriptive like: `AITA_Gemma3_API_Server.ipynb`\n",
    "- Or: `TravelAI_Chat_Demo.ipynb`\n",
    "\n",
    "### Step 3: Runtime Setup\n",
    "- Go to Runtime ‚Üí Change runtime type\n",
    "- Select **T4 GPU** for better performance (recommended)\n",
    "- Click Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77e38af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cell to confirm Google Colab environment is working\n",
    "print(\"‚úÖ Google Colab is working!\")\n",
    "print(\"üöÄ Ready to set up AITA Travel AI Chat API\")\n",
    "\n",
    "# Check Python version\n",
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# Check if we're in Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    print(\"üìç Running in Google Colab\")\n",
    "except ImportError:\n",
    "    print(\"üìç Not running in Google Colab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a667fdea",
   "metadata": {},
   "source": [
    "## üì¶ Step 1: Install Required Packages\n",
    "\n",
    "Run this cell first to install all the necessary packages for the API server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbd4af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install fastapi uvicorn transformers torch pyngrok nest-asyncio accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a26185c",
   "metadata": {},
   "source": [
    "## üìö Step 2: Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a864808",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, Request, HTTPException\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "import nest_asyncio\n",
    "from pyngrok import ngrok\n",
    "import uvicorn\n",
    "import torch\n",
    "\n",
    "# Allow nested event loops (required for Colab)\n",
    "nest_asyncio.apply()\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd049fd",
   "metadata": {},
   "source": [
    "## üöÄ Step 3: Initialize FastAPI App with CORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f586f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize FastAPI app\n",
    "app = FastAPI()\n",
    "\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],  # Allows all origins\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],  # Allows all methods\n",
    "    allow_headers=[\"*\"],  # Allows all headers\n",
    ")\n",
    "\n",
    "print(\"‚úÖ FastAPI app initialized with CORS middleware\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8c1ae3",
   "metadata": {},
   "source": [
    "## üß† Step 4: Load Gemma 3 Model\n",
    "\n",
    "**This will take a few minutes to download and load the model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143e9f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and tokenizer\n",
    "model_name = \"google/gemma-3-1b-it\"\n",
    "print(f\"Loading model: {model_name}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model_name, \n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1,  # Use GPU if available\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Model loaded successfully!\")\n",
    "print(f\"Using device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff7b12f",
   "metadata": {},
   "source": [
    "## üîß Step 5: Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dee174",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_content_text(content):\n",
    "    \"\"\"Extract text from content array format\"\"\"\n",
    "    if isinstance(content, list):\n",
    "        for item in content:\n",
    "            if isinstance(item, dict) and item.get(\"type\") == \"text\":\n",
    "                return item.get(\"text\", \"\")\n",
    "    elif isinstance(content, str):\n",
    "        return content\n",
    "    return \"\"\n",
    "\n",
    "print(\"‚úÖ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6250f067",
   "metadata": {},
   "source": [
    "## üåê Step 6: Define API Endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92074635",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.post(\"/generate\")\n",
    "async def generate(request: Request):\n",
    "    try:\n",
    "        data = await request.json()\n",
    "        messages = data.get(\"messages\", [])\n",
    "        \n",
    "        if not messages:\n",
    "            raise HTTPException(status_code=400, detail=\"No messages provided\")\n",
    "        \n",
    "        # Convert messages to proper format for Gemma 3\n",
    "        formatted_messages = []\n",
    "        for msg in messages:\n",
    "            role = msg.get(\"role\", \"\")\n",
    "            content = msg.get(\"content\", [])\n",
    "            \n",
    "            if role in [\"system\", \"user\", \"assistant\"]:\n",
    "                text_content = extract_content_text(content)\n",
    "                if text_content:\n",
    "                    formatted_messages.append({\n",
    "                        \"role\": role,\n",
    "                        \"content\": text_content\n",
    "                    })\n",
    "        \n",
    "        if not formatted_messages:\n",
    "            raise HTTPException(status_code=400, detail=\"No valid messages found\")\n",
    "        \n",
    "        # Use tokenizer's chat template for proper formatting\n",
    "        try:\n",
    "            # Apply chat template\n",
    "            prompt = tokenizer.apply_chat_template(\n",
    "                formatted_messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "        except Exception as e:\n",
    "            # Fallback to manual formatting if chat template fails\n",
    "            prompt = \"\"\n",
    "            for msg in formatted_messages:\n",
    "                if msg[\"role\"] == \"system\":\n",
    "                    prompt += f\"System: {msg['content']}\\n\"\n",
    "                elif msg[\"role\"] == \"user\":\n",
    "                    prompt += f\"User: {msg['content']}\\n\"\n",
    "                elif msg[\"role\"] == \"assistant\":\n",
    "                    prompt += f\"Assistant: {msg['content']}\\n\"\n",
    "            prompt += \"Assistant:\"\n",
    "        \n",
    "        # Generate response\n",
    "        result = pipe(\n",
    "            prompt, \n",
    "            max_new_tokens=512,  # Reduced for better performance\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        # Extract only the new generated text\n",
    "        generated_text = result[0][\"generated_text\"]\n",
    "        \n",
    "        # Remove the original prompt to get only the new response\n",
    "        if generated_text.startswith(prompt):\n",
    "            response_text = generated_text[len(prompt):].strip()\n",
    "        else:\n",
    "            response_text = generated_text\n",
    "        \n",
    "        return {\"result\": response_text}\n",
    "        \n",
    "    except HTTPException:\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"Error in generate endpoint: {str(e)}\")\n",
    "        raise HTTPException(status_code=500, detail=f\"Internal server error: {str(e)}\")\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    return {\"message\": \"Gemma 3 API is running\"}\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health():\n",
    "    return {\"status\": \"healthy\", \"model\": model_name}\n",
    "\n",
    "print(\"‚úÖ API endpoints defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225f3d70",
   "metadata": {},
   "source": [
    "## üåç Step 7: Start Ngrok Tunnel\n",
    "\n",
    "**Copy the Public URL from the output below and use it in your React Native app!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a564cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start ngrok tunnel\n",
    "try:\n",
    "    # Kill any existing tunnels\n",
    "    ngrok.kill()\n",
    "    \n",
    "    public_url = ngrok.connect(8000)\n",
    "    print(\"üéâ Ngrok tunnel started!\")\n",
    "    print(f\"üîó Public URL: {public_url}\")\n",
    "    print(\"üì± Use this URL in your React Native app!\")\n",
    "    print(\"üìã Copy this URL and replace it in your chatAI.tsx file\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error starting ngrok: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d6d2d5",
   "metadata": {},
   "source": [
    "## üöÄ Step 8: Start FastAPI Server\n",
    "\n",
    "**Run this cell to start the server. It will keep running until you stop it.**\n",
    "\n",
    "**Note:** This cell will block (show a running indicator). The server is working when you see \"Uvicorn running\" message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bff26f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start FastAPI server\n",
    "print(\"üöÄ Starting FastAPI server...\")\n",
    "print(\"üîÑ This cell will run continuously...\")\n",
    "print(\"‚èπÔ∏è Use Runtime > Interrupt execution to stop the server\")\n",
    "\n",
    "uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dbd338",
   "metadata": {},
   "source": [
    "## üß™ Step 9: Test Your API (Optional)\n",
    "\n",
    "Run this cell in a **new tab/window** while the server is running to test if everything works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2324b067",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Test the API with a simple request\n",
    "def test_api():\n",
    "    try:\n",
    "        # Get the current ngrok URL\n",
    "        tunnels = ngrok.get_tunnels()\n",
    "        if tunnels:\n",
    "            url = tunnels[0].public_url\n",
    "            \n",
    "            test_data = {\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": [{\"type\": \"text\", \"text\": \"You are AITA, a helpful AI travel assistant.\"}]\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\", \n",
    "                        \"content\": [{\"type\": \"text\", \"text\": \"Hello! Can you help me plan a trip to Paris?\"}]\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "            print(\"üß™ Testing API...\")\n",
    "            response = requests.post(f\"{url}/generate\", json=test_data, timeout=30)\n",
    "            print(f\"‚úÖ Status Code: {response.status_code}\")\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                print(\"üéâ API Response:\")\n",
    "                print(f\"üìù {result.get('result', 'No result')}\")\n",
    "            else:\n",
    "                print(f\"‚ùå Error: {response.text}\")\n",
    "        else:\n",
    "            print(\"‚ùå No active ngrok tunnels found\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Test failed: {e}\")\n",
    "\n",
    "# Run the test\n",
    "test_api()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af6b795",
   "metadata": {},
   "source": [
    "## üìÆ How to Test with Postman\n",
    "\n",
    "Follow these steps to test your API using Postman:\n",
    "\n",
    "### 1. Setup Postman Request\n",
    "- **Method:** `POST`\n",
    "- **URL:** Use the ngrok URL from Step 7 + `/generate`\n",
    "  - Example: `https://abc123-34-134-35-25.ngrok-free.app/generate`\n",
    "\n",
    "### 2. Set Headers\n",
    "```\n",
    "Content-Type: application/json\n",
    "ngrok-skip-browser-warning: true\n",
    "```\n",
    "\n",
    "### 3. Request Body (Raw JSON)\n",
    "Use this exact format for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b791f138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy this JSON for Postman testing:\n",
    "\n",
    "basic_test = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"You are AITA, a helpful AI travel assistant.\"}]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"What are the best places to visit in Paris?\"}]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Print formatted JSON for easy copying\n",
    "import json\n",
    "print(\"üìã Copy this JSON for Postman:\")\n",
    "print(\"=\" * 50)\n",
    "print(json.dumps(basic_test, indent=2))\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52e7966",
   "metadata": {},
   "source": [
    "### 4. More Test Examples\n",
    "\n",
    "**Travel Planning:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53372a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Travel Planning Test\n",
    "travel_test = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"You are AITA, a helpful AI travel assistant.\"}]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"I'm planning a 5-day trip to Tokyo. What should I include in my itinerary?\"}]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Multi-turn Conversation Test\n",
    "conversation_test = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"You are AITA, a helpful AI travel assistant.\"}]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"I want to visit Europe this summer\"}]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"That sounds exciting! Europe has so much to offer. What type of experience are you looking for - cultural sites, food, nightlife, or outdoor activities?\"}]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"I love historical sites and museums\"}]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"üåç Travel Planning Test:\")\n",
    "print(json.dumps(travel_test, indent=2))\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "print(\"üí¨ Multi-turn Conversation Test:\")\n",
    "print(json.dumps(conversation_test, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2411bb",
   "metadata": {},
   "source": [
    "### 5. Expected Response Format\n",
    "\n",
    "You should get a response like this:\n",
    "```json\n",
    "{\n",
    "  \"result\": \"For a 5-day Tokyo trip, I'd recommend visiting...\"\n",
    "}\n",
    "```\n",
    "\n",
    "### 6. Troubleshooting Common Issues\n",
    "\n",
    "**‚ùå SSL/TLS Error:** Add `ngrok-skip-browser-warning: true` header\n",
    "\n",
    "**‚ùå 404 Not Found:** Make sure your ngrok URL is correct and server is running\n",
    "\n",
    "**‚ùå 500 Internal Server Error:** Check the Colab console for error messages\n",
    "\n",
    "**‚ùå Timeout:** The model might be processing - wait up to 30 seconds\n",
    "\n",
    "**‚ùå CORS Error:** CORS is already configured, this shouldn't happen\n",
    "\n",
    "### 7. Quick Health Check\n",
    "\n",
    "Test this endpoint first to make sure server is running:\n",
    "- **Method:** `GET`  \n",
    "- **URL:** `{your-ngrok-url}/health`"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
